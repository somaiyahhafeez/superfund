# New Jersey's most toxic sites.

## Introduction

Hello! My name is Somaiyah Hafeez and I'm a Data Journalist at Columbia Journalism School. You can find my work [here](https://somaiyahhafeez.github.io) and this project's 
completed story [here](https://somaiyahhafeez.github.io/superfunds/).

For this story I looked into superfund sites, areas that are hazradous and contaminated, either naturally or by companies and where the Environmental Protection Agency (EPA) is working on remediation process. 

You can explore the data [here](https://semspub.epa.gov/work/HQ/406089.pdf). It contains a log of deplatforming events on campus from 1998 to 2025. It's updated regularly by FIRE and while the methodology says that the incidents are self-reported, the reality is that FIRE staffers gather most of these through media reports, 
letters to campuses and student reports. FIRE also employs full fact-checking and independent verification procedures before the incident makes it into the database. 


## Frameworks Used

**Code**
- Python (pandas)
- Google Sheets

**Data Visualization**
- [Datawrapper](https://www.datawrapper.de)
- Google Earth


## Quick Guide To The Files

1. `data` directory: Contains all the main files required for analysis 
    - `superfund.csv` : raw data files
2. `analysis` : main ipynb for analysis 


## Methodology 

Here is the framework that I followed for my analysis. 

1. Using the EPA's National Priorities List for analysis.
    - I used the `superfund.csv` for most of my analysis and merged it with another dataset for population of area of each site.
    - I also used 

    
2. Grouping, Counting and Pivoting data for analysis and visualization

3. Visualization:
    
 - All of the charts and maps were made on DataWraper.

4. Coded a narrative storytelling template using an HTML and CSS framework. Claude was used for the scrollytelling aspect.

## Questions

If you have any questions or if you'd like to chat about my work or otherwise, please [email me!](mailto:sh4625@columbia.edu)


    


